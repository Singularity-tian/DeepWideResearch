"""Research stage strategy for Deep Research.

This module encapsulates the research step, fetching information from
external search providers and returning raw notes.

é‡‡ç”¨åŸºäº Prompt çš„å·¥å…·è°ƒç”¨æ–¹å¼ï¼Œä¸ä½¿ç”¨ OpenAI function call APIã€‚
"""

from __future__ import annotations

import asyncio
import json
import re
import sys
from datetime import datetime
from typing import Dict, List, Optional, Any

# æ”¯æŒç›´æ¥è¿è¡Œå’Œæ¨¡å—å¯¼å…¥
if __name__ == "__main__":
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from deep_wide_research.providers import chat_complete
    from deep_wide_research.mcp_client import get_registry
    from deep_wide_research.newprompt import create_unified_research_prompt
else:
    from .providers import chat_complete
    from .mcp_client import get_registry
    from .newprompt import create_unified_research_prompt


# MCP å·¥å…·é€‰æ‹©é…ç½®ï¼š{server_name: [tool_names]}
MCP_TOOLS_CONFIG = {
    "tavily": ["tavily-search"],
    "exa": ["web_search_exa"]
}

def build_mcp_tools_description(tools: List[Dict[str, Any]]) -> str:
    """æ„å»º MCP å·¥å…·æè¿°ï¼Œç”¨äºæ’å…¥åˆ° unified_research_prompt
    
    Args:
        tools: MCP å·¥å…·åˆ—è¡¨
    
    Returns:
        å·¥å…·æè¿°æ–‡æœ¬
    """
    if not tools:
        return "\n**Note**: No additional search tools are currently available."
    
    tools_description = []
    
    for idx, tool in enumerate(tools, 1):
        tool_name = tool.get("name", "unknown")
        tool_desc = tool.get("description", "No description")
        input_schema = tool.get("inputSchema", {})
        
        tool_text = f"{idx + 1}. **{tool_name}**: {tool_desc}"
        
        properties = input_schema.get("properties", {})
        required = input_schema.get("required", [])
        
        if properties:
            tool_text += "\n   Arguments:"
            for param_name, param_info in properties.items():
                param_type = param_info.get("type", "any")
                param_desc = param_info.get("description", "")
                is_required = "required" if param_name in required else "optional"
                tool_text += f"\n   - {param_name} ({is_required}, {param_type}): {param_desc}"
        
        tools_description.append(tool_text)
    
    tools_list = "\n\n".join(tools_description)
    
    # å·¥å…·è°ƒç”¨æ ¼å¼è¯´æ˜
    example_tool = tools[0]
    example_name = example_tool.get("name", "tool_name")
    example_args = {}
    example_props = example_tool.get("inputSchema", {}).get("properties", {})
    
    for param_name, param_info in list(example_props.items())[:2]:
        param_type = param_info.get("type", "string")
        example_args[param_name] = "example value" if param_type == "string" else (5 if param_type == "integer" else "value")
    
    example_json = json.dumps({"tool": example_name, "arguments": example_args}, indent=2)
    
    return f"""
{tools_list}

**Tool Call Format:**
<tool_call>
{example_json}
</tool_call>

You can call multiple tools in parallel by including multiple <tool_call> blocks."""


def parse_tool_calls(content: str) -> List[Dict[str, Any]]:
    """ä» LLM å“åº”ä¸­è§£æå·¥å…·è°ƒç”¨
    
    ç¤ºä¾‹è¾“å…¥:
    <tool_call>
    {
      "tool": "tavily_search",
      "arguments": {"query": "Python programming", "max_results": 5}
    }
    </tool_call>
    
    è¿”å›: [{"tool": "tavily_search", "arguments": {...}, "id": "call_1"}]
    """
    tool_calls = []
    
    # ä½¿ç”¨æ­£åˆ™æå–æ‰€æœ‰ <tool_call>...</tool_call> å—
    pattern = r'<tool_call>(.*?)</tool_call>'
    matches = re.findall(pattern, content, re.DOTALL)
    
    for idx, match in enumerate(matches):
        try:
            # å°è¯•è§£æ JSON
            tool_data = json.loads(match.strip())
            tool_calls.append({
                "id": f"call_{idx + 1}",
                "tool": tool_data.get("tool", ""),
                "arguments": tool_data.get("arguments", {})
            })
        except json.JSONDecodeError as e:
            print(f"âš ï¸ Failed to parse tool call JSON: {e}")
            continue
    
    return tool_calls


async def _execute_single_tool(
    tc: Dict[str, Any],
    mcp_clients: List
) -> Dict[str, Any]:
    """æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨"""
    result = None
    for client in mcp_clients:
        try:
            result = await client.call_tool(tc["tool"], tc["arguments"])
            result = json.dumps(result)
            break  # æˆåŠŸå°±åœæ­¢
        except:
            continue  # å¤±è´¥å°±å°è¯•ä¸‹ä¸€ä¸ª client
    
    if result is None:
        result = json.dumps({"error": f"Tool '{tc['tool']}' not found in any MCP server"})
    
    # è¾“å‡ºå·¥å…·ç»“æœ
    print(f"\nâœ“ Tool '{tc['tool']}' result ({len(result)} chars)")
    print(f"{'='*60}")
    
    return {
        "tool_call_id": tc["id"],
        "tool": tc["tool"],
        "result": result
    }


async def execute_tool_calls(
    tool_calls: List[Dict[str, Any]],
    mcp_clients: List
) -> List[Dict[str, Any]]:
    """å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨å¹¶è¿”å›ç»“æœåˆ—è¡¨"""
    if not tool_calls:
        return []
    
    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    tool_results = await asyncio.gather(
        *[_execute_single_tool(tc, mcp_clients) for tc in tool_calls]
    )

    print(tool_results)
    
    return list(tool_results)


async def run_research_llm_driven(
    topic: str, 
    cfg, 
    api_keys: Optional[dict] = None,
    mcp_config: Optional[Dict[str, List[str]]] = None,
    deep_param: float = 0.5,
    wide_param: float = 0.5
) -> Dict[str, str]:
    """LLM é©±åŠ¨çš„ç ”ç©¶å¾ªç¯ - ä½¿ç”¨ unified_research_prompt
    
    Args:
        topic: ç ”ç©¶ä¸»é¢˜
        cfg: é…ç½®å¯¹è±¡
        api_keys: API å¯†é’¥å­—å…¸
    """
    if not topic:
        empty_json = json.dumps({"topic": "", "tool_calls": []}, ensure_ascii=False)
        return {"raw_notes": empty_json}
    
    # 1. æ”¶é›† MCP å·¥å…· - ä½¿ç”¨å‰ç«¯ä¼ æ¥çš„é…ç½®æˆ–é»˜è®¤é…ç½®
    print("\nğŸ” Collecting tools from MCP servers...")
    registry = get_registry()
    
    # ä½¿ç”¨å‰ç«¯ä¼ æ¥çš„ MCP é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    effective_config = mcp_config or MCP_TOOLS_CONFIG
    print(f"ğŸ“‹ Using MCP config: {effective_config}")
    
    mcp_tools, mcp_clients = await registry.collect_tools(effective_config)
    
    if not mcp_tools:
        print("âš ï¸ No tools available")
        error_json = json.dumps({
            "topic": topic,
            "tool_calls": [],
            "error": "No tools available"
        }, ensure_ascii=False)
        return {
            "raw_notes": error_json
        }
    
    print(f"âœ… Collected {len(mcp_tools)} tool(s):")
    for tool in mcp_tools:
        print(f"  - {tool.get('name', 'unknown')}")
    
    # 2. æ„å»º system prompt - ä½¿ç”¨ create_unified_research_prompt åŠ¨æ€ç”Ÿæˆ
    mcp_prompt = build_mcp_tools_description(mcp_tools)
    max_iterations = getattr(cfg, 'max_react_tool_calls', 8)
    
    system_prompt = create_unified_research_prompt(
        date=datetime.now().strftime("%Y-%m-%d"),
        mcp_prompt=mcp_prompt,
        max_researcher_iterations=max_iterations,
        deep_param=deep_param,
        wide_param=wide_param
    )
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": topic}
    ]
    
    print(messages)
    
    max_steps = getattr(cfg, 'max_react_tool_calls', 8)
    conversation_history = []  # ä¿å­˜å®Œæ•´å¯¹è¯å†å²ç”¨äºæœ€ç»ˆè¿”å›
    tool_interactions: List[Dict[str, Any]] = []  # ç´¯ç§¯æ‰€æœ‰å·¥å…·è°ƒç”¨åŠç»“æœï¼ˆç”¨äº JSON raw_notesï¼‰

    # å·¥å…·è°ƒç”¨å¾ªç¯
    for step in range(max_steps):
        # è°ƒç”¨ LLMï¼ˆçº¯å¯¹è¯æ¨¡å¼ï¼‰
        resp = await chat_complete(
            model=cfg.research_model,
            messages=messages,
            max_tokens=cfg.research_model_max_tokens,
            api_keys=api_keys,
        )
        
        # è§£æå“åº”ä¸­çš„å·¥å…·è°ƒç”¨
        tool_calls = parse_tool_calls(resp.content)
        
        # è¾“å‡º LLM åŸå§‹å“åº”
        print(f"\n{'='*60}")
        print(f"[Step {step+1}] LLM Output:")
        print(f"{'='*60}")
        print(f"Content:\n{resp.content}")
        if tool_calls:
            print(f"\nğŸ”§ Parsed {len(tool_calls)} tool call(s):")
            for tc in tool_calls:
                print(f"  - {tc['tool']}: {tc['arguments']}")
        print(f"{'='*60}")
        
        # ä¿å­˜åŠ©æ‰‹å“åº”åˆ°å†å²
        conversation_history.append({"role": "assistant", "content": resp.content})
        
        if not tool_calls:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¯´æ˜ LLM å·²ç»ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
            raw_json = json.dumps({
                "topic": topic,
                "tool_calls": tool_interactions,
            }, ensure_ascii=False)
            return {
                "raw_notes": raw_json
            }
        
        # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº† ResearchComplete
        if any(tc["tool"] == "ResearchComplete" for tc in tool_calls):
            print("\nâœ… Research completed by agent")
            return {
                "raw_notes": "\n\n".join([m["content"] for m in conversation_history if m.get("content")])
            }
        
        # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å¯¹è¯
        messages.append({"role": "assistant", "content": resp.content})
        
        # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        tool_results = await execute_tool_calls(tool_calls, mcp_clients)

        # è®°å½•æœ¬è½®å·¥å…·è°ƒç”¨åŠç»“æœï¼ˆç»“æ„åŒ–ä¸º JSON é¡¹ï¼‰
        call_info_map = {tc["id"]: {"tool": tc["tool"], "arguments": tc.get("arguments", {})} for tc in tool_calls}
        for tr in tool_results:
            call_id = tr.get("tool_call_id")
            info = call_info_map.get(call_id, {})
            result_text = tr.get("result", "")
            # ä¼˜å…ˆå°è¯•è§£æä¸º JSON
            parsed_result: Any
            try:
                parsed_result = json.loads(result_text)
            except Exception:
                parsed_result = result_text
            tool_interactions.append({
                "step": step + 1,
                "id": call_id,
                "tool": tr.get("tool") or info.get("tool"),
                "arguments": info.get("arguments", {}),
                "result": parsed_result,
            })
        
        # å°†å·¥å…·ç»“æœæ·»åŠ å›å¯¹è¯
        # æ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æ–‡æœ¬ï¼Œè®© LLM ç†è§£
        results_text = "\n\n".join([
            f"<tool_result tool_call_id=\"{tr['tool_call_id']}\" tool=\"{tr['tool']}\">\n{tr['result']}\n</tool_result>"
            for tr in tool_results
        ])
        
        messages.append({"role": "user", "content": f"Tool results:\n{results_text}"})
        conversation_history.append({"role": "tool_results", "content": results_text})
    
    # è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œè¿”å›å·²æœ‰çš„å†…å®¹
    # è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œè¿”å›å·²æ”¶é›†åˆ°çš„å·¥å…·äº¤äº’ JSON
    raw_json = json.dumps({
        "topic": topic,
        "tool_calls": tool_interactions,
    }, ensure_ascii=False)
    return {"raw_notes": raw_json}


if __name__ == "__main__":
    """åœ¨ VSCode ä¸­ç›´æ¥ç‚¹å‡» Run æŒ‰é’®å³å¯æµ‹è¯•"""
    import asyncio
    
    class TestConfig:
        research_model = "openai/o4-mini"
        research_model_max_tokens = 128000
        max_react_tool_calls = 3
    
    async def test():
        result = await run_research_llm_driven("å¾·å›½å¤§ä¼—çš„è¿‡å»50å¹´çš„å†å²ï¼Œä»¥åŠå®ƒæ—©æœŸçš„å‘å±•è·¯çº¿?", TestConfig())
    
    asyncio.run(test())
